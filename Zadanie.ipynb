{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032ae6a3",
   "metadata": {},
   "source": [
    "## 1. Import bibliotek i ustawienia początkowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac29549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652f598",
   "metadata": {},
   "source": [
    "## 2. Eksploracja i analiza zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(dataset_path: str):\n",
    "    print(f\"--- Analiza zbioru danych w: {dataset_path} ---\")\n",
    "\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        print(f\"\\nBŁĄD: Folder ze zbiorem danych nie został znaleziony pod adresem '{dataset_path}'\")\n",
    "        print(\"Proszę zaktualizować zmienną 'dataset_root' w skrypcie.\")\n",
    "        return\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        dataset = ImageFolder(root=dataset_path, transform=transform)\n",
    "    except (FileNotFoundError, RuntimeError) as e:\n",
    "        print(f\"\\nBŁĄD: Nie można załadować zbioru danych. Sprawdź strukturę folderów.\")\n",
    "        print(f\"Szczegóły błędu: {e}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nZnaleziono {len(dataset)} obrazów.\")\n",
    "\n",
    "    num_classes = len(dataset.classes)\n",
    "    class_names = dataset.classes\n",
    "    print(f\"Liczba klas: {num_classes}\")\n",
    "    print(f\"Nazwy klas: {class_names}\")\n",
    "\n",
    "    print(\"\\nAnaliza rozkładu klas:\")\n",
    "    class_counts = Counter(dataset.targets)\n",
    "    most_common = class_counts.most_common()\n",
    "    print(f\"Najliczniejsza klasa: '{class_names[most_common[0][0]]}' ({most_common[0][1]} obrazów)\")\n",
    "    print(f\"Najmniej liczna klasa: '{class_names[most_common[-1][0]]}' ({most_common[-1][1]} obrazów)\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(class_names, [class_counts[i] for i in range(num_classes)])\n",
    "    plt.title('Rozkład klas w zbiorze danych')\n",
    "    plt.xlabel('Klasa')\n",
    "    plt.ylabel('Liczba obrazów')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nWyświetlanie przykładowych obrazów...\")\n",
    "    sample_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    images, labels = next(iter(sample_loader))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "    fig.suptitle('Przykładowe obrazy ze zbioru danych', fontsize=16)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Konwersja tensora do obrazu numpy\n",
    "        img = images[i].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Klasa: {class_names[labels[i]]}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nAnaliza właściwości obrazów (średnia, odch. standardowe)...\")\n",
    "    loader_stats = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "    for data, _ in loader_stats:\n",
    "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = channels_sum / num_batches\n",
    "    # std = sqrt(E[X^2] - (E[X])^2)\n",
    "    std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "    print(f\"Średnia wartość pikseli (RGB): {mean.tolist()}\")\n",
    "    print(f\"Odchylenie standardowe pikseli (RGB): {std.tolist()}\")\n",
    "    print(\"Wizualna inspekcja przykładowych obrazów jest kluczowa do oceny jakości (ostrość, artefakty, oświetlenie).\")\n",
    "    print(\"\\n--- Koniec analizy ---\")\n",
    "\n",
    "analyze_dataset(DATASET_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b00d30",
   "metadata": {},
   "source": [
    "Zbiór danych jest zbalansowany pod względem liczby próbek w każdej klasie.\n",
    "### Potencjalne problemy\n",
    "\n",
    "- Niektóre obrazy są mało wyraźne — dotyczy to głównie klas `draw` i `stamp`.\n",
    "- Niektóre obiekty typu `stamp` są umieszczone na zakratkowanym tle, co może utrudniać klasyfikację."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0ff1d",
   "metadata": {},
   "source": [
    "## 3. Przygotowanie danych (Preprocessing)\n",
    "\n",
    "Zaimplementowany Preprocesing danych obejmuje:\n",
    "- **Podział na zbiory**: treningowy, walidacyjny i testowy.\n",
    "- **Augmentację danych**: Sztuczne zwiększenie zbioru treningowego poprzez losowe transformacje (obroty i crop), co pomaga modelowi w generalizacji.\n",
    "- **Normalizację**: Przeskalowanie wartości pikseli do standardowego zakresu, co stabilizuje i przyspiesza proces uczenia.\n",
    "- **Redukcję wymiarów**: Zmniejszenie rozmiaru obrazów do 128x128 pikseli, co przyspiesza trening i zmniejsza zużycie pamięci.\n",
    "- **gray-scale**: Konwersję obrazów na czarno-białe w celu uproszczenia danych wejściowych, w których kolory nie są istotne, ponieważ rozpoznajemy kształty na białym tle.\n",
    "- **Utworzenie Dataloderów**: Obiektów, które zarządzają dostarczaniem danych do modelu w trakcie treningu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b39453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader: DataLoader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    print(\"Obliczanie średniej i odchylenia standardowego na zbiorze treningowym...\")\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    print(\"Obliczanie zakończone.\")\n",
    "    return mean, std\n",
    "\n",
    "def create_dataloaders(dataset_path: str, batch_size: int = 32, train_split: float = 0.7, val_split: float = 0.15):\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        print(f\"BŁĄD: Folder ze zbiorem danych nie został znaleziony pod adresem '{dataset_path}'\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    initial_transform = transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.CenterCrop(128),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = ImageFolder(root=dataset_path, transform=initial_transform)\n",
    "    class_names = dataset.classes\n",
    "    print(f\"Znaleziono {len(dataset)} obrazów w {len(class_names)} klasach.\")\n",
    "\n",
    "    test_split = 1 - train_split - val_split\n",
    "    if test_split < 0:\n",
    "        raise ValueError(\"Suma podziałów (train_split, val_split) nie może przekraczać 1.0\")\n",
    "\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = int(val_split * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    print(f\"Podział danych: Treningowe: {len(train_dataset)}, Walidacyjne: {len(val_dataset)}, Testowe: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "    temp_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    mean, std = get_mean_std(temp_train_loader)\n",
    "    print(f\"Obliczona średnia: {mean.tolist()}\")\n",
    "    print(f\"Obliczone odchylenie standardowe: {std.tolist()}\")\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomCrop(128, padding=4),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    val_test_transforms = transforms.Compose([\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "\n",
    "    class TransformedDataset(Subset):\n",
    "        def __init__(self, subset, transform=None):\n",
    "            super().__init__(subset.dataset, subset.indices)\n",
    "            self.transform = transform\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            x, y = super().__getitem__(idx)\n",
    "            if self.transform:\n",
    "                x = self.transform(x)\n",
    "            return x, y\n",
    "\n",
    "    train_dataset = TransformedDataset(train_dataset, train_transforms)\n",
    "    val_dataset = TransformedDataset(val_dataset, val_test_transforms)\n",
    "    test_dataset = TransformedDataset(test_dataset, val_test_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    print(\"\\nDataLoadery zostały pomyślnie utworzone.\")\n",
    "    return train_loader, val_loader, test_loader, class_names\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DATASET_ROOT = './Data'\n",
    "    \n",
    "    train_loader, val_loader, test_loader, class_names = create_dataloaders(DATASET_ROOT)\n",
    "\n",
    "    if train_loader:\n",
    "        print(\"\\nSprawdzanie paczki danych z train_loader...\")\n",
    "        images, labels = next(iter(train_loader))\n",
    "        print(f\"Rozmiar paczki obrazów: {images.shape}\") \n",
    "        print(f\"Rozmiar paczki etykiet: {labels.shape}\")   \n",
    "        print(f\"Przykładowa etykieta: {labels[0].item()} ({class_names[labels[0]]})\")\n",
    "        print(f\"Min/Max wartości pikseli po normalizacji: {images.min():.2f}/{images.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49020c74",
   "metadata": {},
   "source": [
    "## 4. Budowa + trening modelu NN\n",
    "\n",
    "Zdecydowałem się stworzyć model NN z racji tego, że jestem najbardziej zaznajomiony z tą architekturą spośród dostępnych opcji. Również owy model był stosowany do identyfikacji cyfr w zbiorze MNIST, który również składa się z 10 klas, co przekonało mnie do jego użycia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cbe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import create_dataloaders\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1 * 128 * 128, 100)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, return_preds=False):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            if return_preds:\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    \n",
    "    if return_preds:\n",
    "        return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "DATASET_ROOT = './Data'\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 120\n",
    "MODEL_SAVE_PATH = './best_nn_model.pth'\n",
    "train_loader, val_loader, test_loader, class_names = create_dataloaders(DATASET_ROOT, batch_size=BATCH_SIZE)\n",
    "if not train_loader:\n",
    "    print(f\"Brak danych treningowych w {DATASET_ROOT}. Upewnij się, że funkcja create_dataloaders zwraca poprawne DataLoadery.\")\n",
    "    raise SystemExit(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "model = SimpleNN(num_classes=len(class_names)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"\\n--- Rozpoczęcie treningu (Sieć Neuronowa) ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    print(f\"Epoka {epoch+1}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "          f\"Czas: {epoch_duration:.2f}s\")\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Zapisano nowy najlepszy model z dokładnością: {best_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n--- Zakończono trening ---\")\n",
    "print(\"\\n--- Testowanie najlepszego modelu na zbiorze testowym ---\")\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, device, return_preds=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7edf2",
   "metadata": {},
   "source": [
    "## Ewaluacja modelu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96951531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Wyniki na zbiorze testowym -> Strata: {test_loss:.4f}, Dokładność: {test_acc:.4f}\")\n",
    "print(\"\\n--- Raport Klasyfikacji ---\")\n",
    "report_dict = classification_report(test_labels, test_preds, target_names=class_names, output_dict=True)\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names))\n",
    "report_df = pd.DataFrame(report_dict).iloc[:-1, :].T\n",
    "plt.figure(figsize=(10, len(class_names) + 4))\n",
    "sns.heatmap(report_df, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Raport Klasyfikacji')\n",
    "plt.show()\n",
    "print(\"\\n--- Macierz Pomyłek ---\")\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "plt.title('Macierz Pomyłek')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbeaf3",
   "metadata": {},
   "source": [
    "Najlepszy model, jaki udało mi się wytrenować osiągnął maksymalnie około 50-60% dokładności na zbiorze walidacyjnym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8cfe46",
   "metadata": {},
   "source": [
    "### Uzasadnienie wyboru metryk\n",
    "\n",
    "Do oceny modelu zastosowałem następujące metryki, które pozwalają na dokładniejszy wgląd niż accuracy, zwłaszcza, że zbiór testowy nie koniecznie musi być zbalansowany\n",
    "\n",
    "- **Precyzja (Precision)**  \n",
    "  - **Co mierzy?** Jak duży odsetek rysunków, które model oznaczył jako np. \"balon\", to faktycznie balony.  \n",
    "  - Wzór (dla klasy i):  \n",
    "    $$\\text{Precision}_i = \\frac{TP_i}{TP_i + FP_i}$$\n",
    "\n",
    "- **Czułość (Recall)**  \n",
    "  - **Co mierzy?** Jaki odsetek wszystkich faktycznych \"balonów\" w zbiorze testowym model był w stanie poprawnie zidentyfikować.  \n",
    "  - Wzór (dla klasy i):  \n",
    "    $$\\text{Recall}_i = \\frac{TP_i}{TP_i + FN_i}$$\n",
    "\n",
    "- **F1-Score**  \n",
    "  - **Co mierzy?** Średnia harmoniczna precyzji i czułości. Stanowi pojedynczą metrykę, która równoważy oba te wskaźniki.  \n",
    "  - Wzór (dla klasy i):  \n",
    "    $$\\text{F1}_i = 2 \\cdot \\frac{\\text{Precision}_i \\cdot \\text{Recall}_i}{\\text{Precision}_i + \\text{Recall}_i} = \\frac{2 TP_i}{2 TP_i + FP_i + FN_i}$$\n",
    "\n",
    "- **Macro Average F1-Score (`macro-avg`)**  \n",
    "  - Oblicza F1-Score dla każdej klasy osobno, a następnie wyciąga z nich prostą średnią. Każda klasa ma taką samą wagę.  \n",
    "  - Wzór:  \n",
    "    $$\\text{F1}_{\\text{macro}} = \\frac{1}{K}\\sum_{i=1}^{K} \\text{F1}_i$$\n",
    "    gdzie K to liczba klas.\n",
    "\n",
    "- **Weighted Average F1-Score (`weighted-avg`)**  \n",
    "  - Podobna do `macro-avg`, ale przy uśrednianiu bierze pod uwagę liczebność każdej klasy (`support`). Klasy z większą liczbą próbek mają większy wpływ na końcowy wynik.  \n",
    "  - Wzór:  \n",
    "    $$\\text{F1}_{\\text{weighted}} = \\frac{1}{N}\\sum_{i=1}^{K} s_i \\cdot \\text{F1}_i$$\n",
    "    gdzie \\(s_i\\) to support (liczba próbek klasy i), a \\(N=\\sum_{i=1}^{K} s_i\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54cfc4f",
   "metadata": {},
   "source": [
    "## 4. Budowa i trening modelu CNN\n",
    "\n",
    "Z racji tego, że nie jestem usatysfakcjonowany wynikami uzyskanymi przez model NN, zdecydowałem się na implementację nowego modelu CNN, który jest moim pierszym zetknięciem z tą architekturą(musiałem się w między czasie doszkolić z dziłania CNN).\n",
    "\n",
    "CNN powinien lepiej radzić sobie z rozpoznawaniem obrazów dzięki swojej zdolności do wykrywania cech przestrzennych w odróżnieniu od standardowego modelu NN, zwłaszcza, że obrazy w naszym zbiorze danych zawierają różnorodne kształty i wzory, które mogą być lepiej uchwycone przez warstwy konwolucyjne. Także zakratkowane tło w niektórych obrazach może być lepiej zignorowane przez CNN, które potrafią skupić się na istotnych cechach obrazu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2465df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Blok 1: Wejście (1, 128, 128) -> Wyjście (16, 64, 64)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Blok 2: Wejście (16, 64, 64) -> Wyjście (32, 32, 32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Blok 3: Wejście (32, 32, 32) -> Wyjście (32, 16, 16)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Rozmiar wejściowy: 32 kanały * 16 * 16 (po ostatnim poolingu)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 80)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(80, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71f1b25",
   "metadata": {},
   "source": [
    "## Trening CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b90947",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = './Data'\n",
    "BATCH_SIZE = 35\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 35\n",
    "MODEL_SAVE_PATH = './best_model.pth'\n",
    "\n",
    "train_loader, val_loader, test_loader, class_names = create_dataloaders(DATASET_ROOT, batch_size=BATCH_SIZE)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "\n",
    "model_cnn = SimpleCNN(num_classes=len(class_names)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"\\n--- Rozpoczęcie treningu CNN ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model_cnn, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model_cnn, val_loader, criterion, device)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "\n",
    "    print(f\"Epoka {epoch+1}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n",
    "          f\"Czas: {epoch_duration:.2f}s\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_cnn.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Zapisano nowy najlepszy model z dokładnością: {best_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n--- Zakończono trening ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5a6fd",
   "metadata": {},
   "source": [
    "## 5. Ewaluacja modelu CNN\n",
    "Metryki użyte do oceny modelu CNN są takie same jak w przypadku modelu NN, z tych samych powodów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testowanie najlepszego modelu na zbiorze testowym ---\")\n",
    "model_cnn.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(model_cnn, test_loader, criterion, device, return_preds=True)\n",
    "print(f\"Wyniki na zbiorze testowym -> Strata: {test_loss:.4f}, Dokładność: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n--- Raport Klasyfikacji ---\")\n",
    "report_dict = classification_report(test_labels, test_preds, target_names=class_names, output_dict=True)\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names))\n",
    "report_df = pd.DataFrame(report_dict).iloc[:-1, :].T\n",
    "plt.figure(figsize=(10, len(class_names) + 4))\n",
    "sns.heatmap(report_df, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Raport Klasyfikacji')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Macierz Pomyłek ---\")\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "plt.xlabel('Przewidziana etykieta')\n",
    "plt.ylabel('Prawdziwa etykieta')\n",
    "plt.title('Macierz Pomyłek')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805711fe",
   "metadata": {},
   "source": [
    "## 7. Porównanie z modelem alternatywnym (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT_RF = './Data'\n",
    "BATCH_SIZE_RF = 64\n",
    "\n",
    "\n",
    "train_loader_rf, _, test_loader_rf, class_names_rf = create_dataloaders(\n",
    "    DATASET_ROOT_RF, \n",
    "    batch_size=BATCH_SIZE_RF,\n",
    "    train_split=0.8, \n",
    "    val_split=0.0\n",
    ")\n",
    "\n",
    "def get_data_from_loader(loader):\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Przetwarzanie danych z {type(loader.dataset).__name__}...\")\n",
    "    for images, lbls in loader:\n",
    "        # Spłaszczenie obrazów z (batch, channels, height, width) do (batch, features)\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        features.append(images.numpy())\n",
    "        labels.append(lbls.numpy())\n",
    "        \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    print(\"Przetwarzanie zakończone.\")\n",
    "    return features, labels\n",
    "\n",
    "# Konwersja danych z DataLoaderów do formatu NumPy dla Scikit-learn\n",
    "X_train_rf, y_train_rf = get_data_from_loader(train_loader_rf)\n",
    "X_test_rf, y_test_rf = get_data_from_loader(test_loader_rf)\n",
    "\n",
    "print(f\"\\nRozmiar danych treningowych: {X_train_rf.shape}\")\n",
    "print(f\"Rozmiar danych testowych: {X_test_rf.shape}\")\n",
    "\n",
    "print(\"\\n--- Rozpoczęcie trenowania Random Forest ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1, oob_score=True)\n",
    "rf_classifier.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "training_duration = time.time() - start_time\n",
    "print(f\"--- Zakończono trening w {training_duration:.2f}s ---\")\n",
    "print(f\"Dokładność: {rf_classifier.oob_score_:.4f}\")\n",
    "\n",
    "print(\"\\n--- Testowanie modelu na zbiorze testowym ---\")\n",
    "start_time = time.time()\n",
    "y_pred_rf = rf_classifier.predict(X_test_rf)\n",
    "prediction_duration = time.time() - start_time\n",
    "\n",
    "test_acc_rf = np.mean(y_pred_rf == y_test_rf)\n",
    "print(f\"Czas predykcji: {prediction_duration:.2f}s\")\n",
    "print(f\"Dokładność na zbiorze testowym: {test_acc_rf:.4f}\")\n",
    "\n",
    "print(\"\\n--- Raport Klasyfikacji ---\")\n",
    "report_dict_rf = classification_report(y_test_rf, y_pred_rf, target_names=class_names_rf, output_dict=True)\n",
    "print(classification_report(y_test_rf, y_pred_rf, target_names=class_names_rf))\n",
    "\n",
    "report_df_rf = pd.DataFrame(report_dict_rf).iloc[:-1, :].T\n",
    "plt.figure(figsize=(10, len(class_names_rf) + 4))\n",
    "sns.heatmap(report_df_rf, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Raport Klasyfikacji - Random Forest')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Macierz Pomyłek ---\")\n",
    "cm_rf = confusion_matrix(y_test_rf, y_pred_rf)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', xticklabels=class_names_rf, yticklabels=class_names_rf, cmap='Blues')\n",
    "plt.title('Macierz Pomyłek - Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289014be",
   "metadata": {},
   "source": [
    "## 8. Podsumowanie\n",
    "Z zaimplementowanych modeli najlepsze wyniki osiągnął model CNN. \n",
    "Podczas implementacji napotkałem się z rónymi błędami, przeważnie wynikającymi z nieznajomości bibliotek lub nieodpowiedniego użycia funkcji, jak np. zapomniałem dodać optimizer.zero_grad() przed loss.backward() w treningu modelu NN, co skutkowało niepoprawnym aktualizowaniem wag modelu.\n",
    "\n",
    "### Augumentacja danych\n",
    "Zauważyłem, również, że augmentacja danych pomogła zapobiec overfittingowi, podczas treningu modeli, modele bez augmentacji osiągały gorszy stosunek pomiędzy dokładnością na zbiorze treningowym i walidacyjnym.\n",
    "\n",
    "\n",
    "### Miejsca do poprawy\n",
    "- dalsza optymalizacja architektury modeli i hiperparametrów,\n",
    "- eksperymenty z różnymi technikami augmentacji danych,\n",
    "- wprowadzenie wizualizacji procesu treningu (np. wykresy strat i dokładności w czasie)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
